{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pandas\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_SUBSETS = 385"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Combine all of the csv files\n",
    "print('1. Concatenating all of the csv files...')\n",
    "clicks_csv = [pandas.read_csv(f'archive/clicks/clicks/clicks_hour_{str(hour).zfill(3)}.csv') for hour in range(NUMBER_OF_SUBSETS)]\n",
    "all_clicks = pandas.concat(clicks_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 1. Save the combined csv file\n",
    "print('--- Saving checkpoint 1. Combined csv file...')\n",
    "all_clicks.to_csv('globo_all_clicks.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Only keep relevant columns\n",
    "print('2. Only keeping relevant columns...')\n",
    "only_relevant_columns = all_clicks[['session_id', 'session_size', 'click_article_id', 'click_timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Discard sessions with less than 2 clicks\n",
    "print('3. Discarding sessions with less than 2 clicks...')\n",
    "all_clicks_with_at_least_2_clicks = only_relevant_columns.query('session_size > 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Normalize item ids starting from 1\n",
    "print('4. Normalizing item ids starting from 1...')\n",
    "\n",
    "# Sort the values in ascending order\n",
    "all_clicks_with_at_least_2_clicks.sort_values(by='click_article_id')\n",
    "\n",
    "# Create a new column with integers representing unique values starting from 1\n",
    "all_clicks_with_at_least_2_clicks['item_id'] = all_clicks_with_at_least_2_clicks['click_article_id'].astype('category').cat.codes + 1\n",
    "\n",
    "# Drop the original column\n",
    "all_clicks_with_at_least_2_clicks.drop('click_article_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint 2. Save the normalized csv file\n",
    "print('--- Saving checkpoint 2. Normalized csv file...')\n",
    "all_clicks_with_at_least_2_clicks.to_csv('globo_normalized_items.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create master sequence file\n",
    "print('5. Creating master sequence file...')\n",
    "\n",
    "# Retrieve from the checkpoint\n",
    "globo_normalized_items = pandas.read_csv('globo_normalized_items.csv')\n",
    "\n",
    "# Rename the click_timestamp column to timestamp\n",
    "globo_normalized_items.rename(columns={'click_timestamp': 'timestamp'}, inplace=True)\n",
    "\n",
    "# Same mapping for the session_id column but with integers starting from 0\n",
    "# This is done in order to use the integers as indices for the sessions\n",
    "globo_normalized_items.sort_values(by='session_id')\n",
    "globo_normalized_items['session'] = globo_normalized_items['session_id'].astype('category').cat.codes\n",
    "globo_normalized_items.drop('session_id', axis=1, inplace=True)\n",
    "\n",
    "# Grabbing the number of sessions\n",
    "number_of_sessions = globo_normalized_items.iloc[-1]['session'] + 1\n",
    "\n",
    "print(f'Number of sessions: {number_of_sessions}')\n",
    "\n",
    "with open('globo_sequences.txt', 'w') as f:\n",
    "    # For each session, sort its timestamps in ascending order\n",
    "    for session in tqdm(range(number_of_sessions)):\n",
    "        session_data = globo_normalized_items.query(f'session == {session}')\n",
    "        session_data.sort_values(by='timestamp', inplace=True)\n",
    "        \n",
    "        # Then grab its item ids and put them in a list\n",
    "        session_sequence = session_data['item_id'].values.tolist()\n",
    "        \n",
    "        # Append session_sequence to a file containing all of the session's subsequences*. The items are separated by a comma\n",
    "        # and the sessions are separated by a new line\n",
    "        for subsequence in range(2, len(session_sequence) + 1):\n",
    "            subsequence = ','.join(map(str, session_sequence[:subsequence]))\n",
    "            f.write(f'{subsequence}\\n')\n",
    "            \n",
    "        # * A subsequence in this context starts from the beginning and is a subset of the current sequence.\n",
    "        # For example, the sequence [1, 2, 3, 4] has the subsequences [1, 2], [1, 2, 3], [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(filename, ratio=0.9):\n",
    "    with open(filename, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    # Split the dataset into a training set and a testing set\n",
    "    random.shuffle(lines)\n",
    "    num_lines = len(lines)\n",
    "    training_set_size = int(ratio * num_lines)\n",
    "    training_set = lines[:training_set_size]\n",
    "    testing_set = lines[training_set_size:]\n",
    "\n",
    "    # Create a set of items in the training set\n",
    "    item_set = set()\n",
    "    for line in training_set:\n",
    "        items = line.strip().split(\",\")\n",
    "        item_set |= set(items) # item_set.union(set(items))\n",
    "\n",
    "    # Filter out the sequences that contain items that are not in the training set\n",
    "    filtered_testing_set = []\n",
    "    for line in testing_set:\n",
    "        items = line.strip().split(\",\")\n",
    "        if set(items).isdisjoint(item_set):\n",
    "            filtered_testing_set.append(line)\n",
    "\n",
    "    return training_set, filtered_testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Splitting the dataset into a training set and a testing set\n",
    "print(\"6. Splitting the dataset into a training set and a testing set...\")\n",
    "\n",
    "training_set, testing_set = split_dataset('globo_sequences.txt')\n",
    "\n",
    "with open('globo_training_set.txt', 'w') as f:\n",
    "    f.writelines(training_set)\n",
    "\n",
    "with open('globo_testing_set.txt', 'w') as f:\n",
    "    f.writelines(testing_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
